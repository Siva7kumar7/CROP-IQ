"""
UPGRADED HYBRID PLANT DISEASE DETECTION
âœ” High accuracy
âœ” Laptop safe
âœ” CNN + SVM + Severity Regression
"""

# =========================
# IMPORTS
# =========================
import os, json, warnings
warnings.filterwarnings("ignore")

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

import numpy as np
import pandas as pd
import cv2
import joblib

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# =========================
# CONFIG
# =========================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = r"D:\final_project\backend\plant_disease\plant_datas"
SEVERITY_CSV = os.path.join(BASE_DIR, "severity_labels.csv")

IMG_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS_STAGE1 = 5
EPOCHS_STAGE2 = 5
VAL_SPLIT = 0.2
SEED = 42

AUTOTUNE = tf.data.AUTOTUNE

# =========================
# SAFETY CHECK
# =========================
if not os.path.exists(DATASET_DIR):
    raise FileNotFoundError(f"Dataset not found: {DATASET_DIR}")

# =========================
# LOAD DATA
# =========================
train_ds = tf.keras.utils.image_dataset_from_directory(
    DATASET_DIR,
    validation_split=VAL_SPLIT,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DATASET_DIR,
    validation_split=VAL_SPLIT,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
num_classes = len(class_names)

# Save class mapping
with open("class_indices.json", "w") as f:
    json.dump({c: i for i, c in enumerate(class_names)}, f, indent=2)

# =========================
# DATA AUGMENTATION
# =========================
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.15),
    layers.RandomZoom(0.15),
    layers.RandomContrast(0.15),
])

train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))
train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y)).prefetch(AUTOTUNE)
val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y)).prefetch(AUTOTUNE)

# =========================
# CLASS WEIGHTS
# =========================
class_counts = {
    cls: len(os.listdir(os.path.join(DATASET_DIR, cls)))
    for cls in class_names
}

total = sum(class_counts.values())
class_weights = {
    i: total / (class_counts[class_names[i]] + 1e-6)
    for i in range(num_classes)
}

print("âœ… Class weights ready")

# =========================
# CNN MODEL
# =========================
base_model = MobileNetV2(
    include_top=False,
    weights="imagenet",
    input_shape=(224, 224, 3)
)
base_model.trainable = False

inputs = layers.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation="relu", name="feature_layer")(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)

cnn_model = models.Model(inputs, outputs)

cnn_model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# =========================
# STAGE 1 TRAINING
# =========================
print("\nðŸ”¹ Stage 1: Training classifier head")
cnn_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS_STAGE1,
    class_weight=class_weights
)

# =========================
# FINE-TUNING
# =========================
base_model.trainable = True
for layer in base_model.layers[:-30]:
    layer.trainable = False

cnn_model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print("\nðŸ”¹ Stage 2: Fine-tuning CNN")
cnn_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS_STAGE2
)

# =========================
# SAVE CNN
# =========================
cnn_model.save("plant_disease_classifier.h5", include_optimizer=False)
print("âœ… CNN MODEL SAVED")

# =========================
# FEATURE EXTRACTOR
# =========================
feature_extractor = models.Model(
    cnn_model.input,
    cnn_model.get_layer("feature_layer").output
)

# =========================
# SVM TRAINING
# =========================
X_feat, y_lab = [], []

for i, (images, labels) in enumerate(train_ds):
    feats = feature_extractor.predict(images, verbose=0)
    X_feat.append(feats)
    y_lab.append(labels.numpy())
    if i >= 150:
        break

X_feat = np.vstack(X_feat)
y_lab = np.concatenate(y_lab)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_feat)

svm = SVC(kernel="rbf", C=10, gamma="scale", probability=True)
svm.fit(X_scaled, y_lab)

joblib.dump(svm, "svm_classifier.pkl")
joblib.dump(scaler, "svm_scaler.pkl")
print("âœ… SVM MODEL SAVED")

# =========================
# SEVERITY REGRESSION
# =========================
if os.path.exists(SEVERITY_CSV):
    df = pd.read_csv(SEVERITY_CSV)

    X_reg, y_reg = [], []

    for _, r in df.iterrows():
        img_path = r["image_path"]
        if not os.path.exists(img_path):
            continue

        img = cv2.imread(img_path)
        img = cv2.resize(img, IMG_SIZE)
        img = preprocess_input(img)
        img = np.expand_dims(img, axis=0)

        feat = feature_extractor.predict(img, verbose=0)[0]
        X_reg.append(feat)
        y_reg.append(float(r["severity"]))

        if len(X_reg) >= 200:
            break

    if len(X_reg) >= 10:
        reg = LinearRegression()
        reg.fit(X_reg, y_reg)
        joblib.dump(reg, "severity_regressor.pkl")
        print("âœ… Severity regressor saved")

print("\nðŸŽ¯ TRAINING COMPLETE â€“ HIGH ACCURACY MODELS READY")
